# -*- coding: UTF-8 -*-
import datasets

from data.CONLLReader import CONLLReader
from classification.Classifier import Classifier, Task, MultitaskClassifier
from tokenization import Tokenization
import unicodedata as ud
import os
from transformers import AutoConfig
import argparse
from argparse import ArgumentParser
from lexicon.LexiconProcessor import LexiconProcessor
from data import Datasets
from tqdm import tqdm
import json

class Tagger:

    def __init__(self, transformer_path, model_dir, tokenizer_path=None, training_data=None, test_data=None,
                 lexicon_file=None, lexicon_dict=None, possible_tags_file=None, data_preset='CONLLU', feature_cols=None, feats=['UPOS', 'XPOS', 'FEATS'],
                 unknown_label=None, add_training_data_to_possible_tags=True, add_training_data_to_lexicon=True,
                 normalization_rule=None, is_multitask=False):
        self.reader = CONLLReader(data_preset,feature_cols)
        self.transformer_path = transformer_path
        self.tokenizer_path = tokenizer_path
        if tokenizer_path == None:
            self.tokenizer_path = self.transformer_path
        self.feats = [feat.lstrip() for feat in feats]
        self.model_dir = model_dir
        self.unknown_label = unknown_label
        if training_data is not None:
            self.training_data = self.reader.parse_conll(training_data)
        else:
            self.training_data = None
        if test_data is not None:
            self.test_data = self.reader.parse_conll(test_data)
        else:
            self.test_data = None
        self.feature_dict = self.build_feature_dict()
        if lexicon_file is not None or lexicon_dict is not None:
            if lexicon_file is not None:
                self.lexicon = self.read_lexicon(lexicon_file,normalization_rule)
            else:
                self.lexicon = lexicon_dict
            lp = LexiconProcessor(self.lexicon)
            lp.trim_lexicon(self.feature_dict)
            self.lexicon = lp.lexicon
            if add_training_data_to_lexicon and self.training_data is not None:
                lp.add_data(self.training_data, feats=self.feature_dict, col_token=self.reader.feature_cols['FORM'],
                            col_upos=self.reader.feature_cols['UPOS'], col_xpos=self.reader.feature_cols['XPOS'],
                            col_morph=self.reader.feature_cols['FEATS'], normalization_rule=normalization_rule,
                            reader_preset=self.reader.preset)
        else:
            self.lexicon = None
        self.add_training_data_to_possible_tags = add_training_data_to_possible_tags
        self.possible_tags = self.build_possible_tags(possible_tags_file)
        self.normalization_rule = normalization_rule
        self.is_multitask = is_multitask

    def tag_individual_feat(self, feat, test_data, batch_size=16):
        if not self.is_multitask:
            feature_classifier = Classifier(self.transformer_path, tokenizer_path=self.tokenizer_path,
                                            unknown_label=self.unknown_label, model_dir=self.model_dir)
            preds = feature_classifier.predict(test_data, model_dir=f"{self.model_dir}/{feat}", batch_size=batch_size,
                                               labelname=feat)
        else:
            feature_classifier = MultitaskClassifier(self.transformer_path, tokenizer_path=self.tokenizer_path,
                                                 unknown_label=self.unknown_label, model_dir=self.model_dir)
            feature_classifier.tasks = self.tasks
            preds = feature_classifier.predict(test_data, model_dir=self.model_dir, batch_size=batch_size,
                                               labelname=feat)

        return (feat, preds)

    def tag_seperately(self, tokens, batch_size=16, tokenizer_add_prefix_space=False, print_prediction=True):
        tag_dict = {}
        for feat in self.feature_dict:
            tags = []
            if self.test_data is not None:
                if feat == 'UPOS' or feat == 'XPOS':
                    tags = self.reader.read_tokens(self.test_data, feat, return_wids=False, return_tokens=False)
                else:
                    tags = self.reader.read_tokens(self.test_data, feat, in_feats=True, return_wids=False,
                                                 return_tokens=False)
            else:
                for sent in tokens:
                    tag_sent = []
                    for word in sent:
                        tag_sent.append(self.unknown_label)
                    tags.append(tag_sent)
            tag_dict[feat] = tags
        test_data = Datasets.build_dataset(tokens, tag_dict)
        # This is not very elegant, but tokenized_string is 'locked behind' classifier. Maybe it's better to move to
        # another class e.g. Tokenization? I'm not sure if this is the best way to do so though since this contains
        # methods not specific for transformers.
        if not self.is_multitask:
            classifier = Classifier(self.transformer_path, None, self.tokenizer_path,
                                    tokenizer_add_prefix_space=tokenizer_add_prefix_space)
        else:
            classifier = MultitaskClassifier(self.transformer_path, None, self.tokenizer_path,
                                         tokenizer_add_prefix_space=tokenizer_add_prefix_space)
        test_data = test_data.map(Tokenization.tokenize_sentence, fn_kwargs={"tokenizer": classifier.tokenizer})
        all_preds = {}
        self.tasks = []
        for feat_idx, feat in enumerate(self.feature_dict):
            if self.is_multitask:
                if feat == 'UPOS' or feat == 'XPOS':
                    tags = self.reader.read_tokens(self.training_data, feat, return_wids=False, return_tokens=False)
                else:
                    tags = self.reader.read_tokens(self.training_data, feat, in_feats=True, return_wids=False,
                                                 return_tokens=False)
                tag2id, id2tag = classifier.id_label_mappings(tags)
                task_info = Task(
                    id=feat_idx,
                    name=feat,
                    num_labels=len(tag2id),
                    label_list=list(tag2id.keys()),
                    type="token_classification" if feat != "dephead" else "question_answering"
                )
                self.tasks.append(task_info)

            result = self.tag_individual_feat(feat, test_data, batch_size=batch_size)
            all_preds[result[0]] = result[1]
            if print_prediction:
                print('Predicted ' + feat)
        return all_preds

    def train_models(self, tokens, batch_size=16, epochs=3, learning_rate=5e-5, freeze_epochs=0, normalization_rule=None,
                     tokenizer_add_prefix_space=False):
        tokens_norm = tokens
        if normalization_rule is not None:
            tokens_norm = Tokenization.normalize_tokens(tokens, normalization_rule)

        if not self.is_multitask:
            feat_classifier = Classifier(transformer_path=self.transformer_path, model_dir=self.model_dir,
                                         tokenizer_path=self.tokenizer_path,
                                         tokenizer_add_prefix_space=tokenizer_add_prefix_space)
        else:
            feat_classifier = MultitaskClassifier(transformer_path=self.transformer_path, model_dir=self.model_dir,
                                              tokenizer_path=self.tokenizer_path,
                                              tokenizer_add_prefix_space=tokenizer_add_prefix_space)

        tag_dict = {}
        tag2id_all = {}
        id2tag_all = {}
        for feat in tqdm(self.feature_dict, desc="Reading feat data"):
            if feat == 'UPOS' or feat == 'XPOS':
                tags = self.reader.read_tokens(self.training_data, feat, return_wids=False, return_tokens=False)
            else:
                tags = self.reader.read_tokens(self.training_data, feat, in_feats=True, return_wids=False,
                                             return_tokens=False)
            tag_dict[feat] = tags
            tag2id, id2tag = feat_classifier.id_label_mappings(tags)
            tag2id_all[feat] = tag2id
            id2tag_all[feat] = id2tag
        training_data = Datasets.build_dataset(tokens_norm, tag_dict)
        training_data = training_data.map(Tokenization.tokenize_sentence,
                                          fn_kwargs={"tokenizer": feat_classifier.tokenizer})

        if not self.is_multitask:
            for feat in self.feature_dict:
                training_data_feat = training_data.map(feat_classifier.align_labels,
                                                       fn_kwargs={"tag2id": tag2id_all[feat], "labelname": feat})
                feat_classifier.train_classifier(f"{self.model_dir}/{feat}", training_data_feat,
                                                 tag2id=tag2id_all[feat], id2tag=id2tag_all[feat],
                                                 batch_size=batch_size,
                                                 epochs=epochs,learning_rate=learning_rate, freeze_epochs=freeze_epochs)
        else:
            training_data_df = training_data.to_pandas()  # once huggingface implements selecting columns, use datasets directly
            multitask_feat_datasets = []
            self.tasks = []

            for feat_idx, feat in tqdm(enumerate(self.feature_dict)):
                task_info = Task(
                    id=feat_idx,
                    name=feat,
                    num_labels=len(tag2id_all[feat]),
                    label_list=list(tag2id_all[feat].keys()),
                    type="token_classification" if feat != "dephead" else "question_answering"
                )
                self.tasks.append(task_info)

# Should be rewritten to use Dataset.map as for the non-multitask classifier
                feat_df = training_data_df[["tokens", task_info.name, 'input_ids', 'attention_mask','subword_ids']]
                feat_df = feat_df.assign(task_ids=task_info.id)

                training_data_feat = Datasets.Dataset.from_pandas(feat_df).map(feat_classifier.align_labels,
                                                                               fn_kwargs={
                                                                                   "tag2id": tag2id_all[task_info.name],
                                                                                   "labelname": task_info.name})
                training_data_feat = training_data_feat.remove_columns([task_info.name])
                multitask_feat_datasets.append(training_data_feat)

            multitask_feat_data = datasets.concatenate_datasets(multitask_feat_datasets)
            feat_classifier.tasks = self.tasks
            feat_classifier.train_classifier(f"{self.model_dir}", multitask_feat_data,
                                             tag2id=tag2id_all, id2tag=id2tag_all, batch_size=batch_size,
                                             epochs=epochs,learning_rate=learning_rate, freeze_epochs=freeze_epochs)

    def build_feature_dict(self):
        # Builds a dictionary with all tagging features and their possible values, based on the training data or the
        # saved tagger models. This might be unnecessary: it is not clear to me anymore why we need the
        # possible_values instead of only the names of the features. Maybe just for diagnostic purposes?
        feature_dict = dict()
        if self.training_data is not None:
            for sent in self.training_data:
                for word in sent:
                    if 'UPOS' in self.feats:
                        current_upos = word[self.reader.feature_cols['UPOS']]
                        if 'UPOS' in feature_dict:
                            upos_values = feature_dict['UPOS']
                            upos_values.add(current_upos)
                        else:
                            upos_values = set()
                            upos_values.add(current_upos)
                            feature_dict['UPOS'] = upos_values
                    if 'XPOS' in self.feats:
                        current_xpos = word[self.reader.feature_cols['XPOS']]
                        if 'XPOS' in feature_dict:
                            xpos_values = feature_dict['XPOS']
                            xpos_values.add(current_xpos)
                        else:
                            xpos_values = set()
                            xpos_values.add(current_xpos)
                            feature_dict['XPOS'] = xpos_values
                    if self.reader.preset == "CONLLU":
                        for key, val in word[self.reader.feature_cols['FEATS']].items():
                            if 'FEATS' in self.feats or key in self.feats:
                                if key in feature_dict:
                                    feat_values = feature_dict[key]
                                    feat_values.add(val)
                                    feature_dict[key] = feat_values
                                else:
                                    feat_values = set()
                                    feat_values.add(val)
                                    feature_dict[key] = feat_values
                    elif word[self.reader.feature_cols['FEATS']] != '_':
                        morph = word[self.reader.feature_cols['FEATS']].split('|')
                        for feat in morph:
                            split_feat = feat.split('=')
                            if 'FEATS' in self.feats or split_feat[0] in self.feats:
                                if split_feat[0] in feature_dict:
                                    feat_values = feature_dict[split_feat[0]]
                                    feat_values.add(split_feat[1])
                                    feature_dict[split_feat[0]] = feat_values
                                else:
                                    feat_values = set()
                                    feat_values.add(split_feat[1])
                                    feature_dict[split_feat[0]] = feat_values
        elif self.model_dir is not None:
            for file in os.listdir(self.model_dir):
                path = os.path.join(self.model_dir, file)
                if os.path.isdir(path) and 'config.json' in os.listdir(path) and (('FEATS' in self.feats and file != 'UPOS' and file != 'XPOS') or file in self.feats):
                    config = AutoConfig.from_pretrained(path)
                    feature_dict[file] = set(config.label2id.keys())
        return feature_dict

    def build_possible_tags(self, possible_tags_file):
        possible_tags = []
        if possible_tags_file is not None:
            file = open(possible_tags_file, encoding='utf-8')
            raw_text = file.read().strip()
            lines = raw_text.split('\n')
            header = lines.pop(0).split('\t')
            feat_col = {}
            for col, feat in enumerate(header):
                feat_col[feat] = col
            for line in lines:
                tag = []
                vals = line.split('\t')
                for feat in self.feature_dict:
                    tag.append((feat, vals[feat_col[feat]]))
                tag = tuple(tag)
                possible_tags.append(tag)
        if self.training_data is not None and self.add_training_data_to_possible_tags:
            for sent in tqdm(self.training_data, desc="Building possible tags"):
                for word in sent:
                    tag = []
                    if 'FEATS' in self.reader.feature_cols:
                        feats = word[self.reader.feature_cols['FEATS']]
                        if self.reader.preset != 'CONLLU':
                            feats_split = feats.split('|')
                    for feat in self.feature_dict:
                        if feat == 'UPOS':
                            tag.append((feat, word[self.reader.feature_cols['UPOS']]))
                        elif feat == 'XPOS':
                            tag.append((feat, word[self.reader.feature_cols['XPOS']]))
                        else:
                            if self.reader.preset == 'CONLLU':
                                if not feat in feats:
                                    tag.append((feat, '_'))
                                else:
                                    tag.append((feat,feats[feat]))
                            else:
                                if feats == '_':
                                    tag.append((feat, '_'))
                                else:
                                    found = False
                                    for feat_val in feats_split:
                                        feat_val_split = feat_val.split('=')
                                        if feat_val_split[0] == feat:
                                            tag.append((feat_val_split[0], feat_val_split[1]))
                                            found = True
                                            break
                                    if not found:
                                        tag.append((feat, '_'))
                    tag = tuple(tag)
                    if tag not in possible_tags:
                        possible_tags.append(tag)
        elif possible_tags_file is None:
            return None
        return possible_tags

    def calc_tag_probs(self, possible_tags, preds, word_no):
        tag_probs = {}
        for tag in possible_tags:
            tag_prob = 1
            for feat in tag:
                try:
                    prob_attr = preds[feat[0]][word_no][feat[1]]
                except KeyError:
                    prob_attr = 0
                    print('Feature not found (probably mismatch with lexicon): ' + feat[0] + ' ' + feat[1])
                except IndexError:
                    print(word_no)
                    print(len(preds[feat[0]]))
                tag_prob *= prob_attr
            tag_probs[tag] = tag_prob
        tag_probs = sorted(tag_probs.items(), reverse=True, key=lambda x: x[1])
        return tag_probs

    def tag_data(self, tokens, preds, return_all_probs=False, return_num_poss=False, disable_progress_bar=False):
        best_tags = []
        all_tags = []
        num_poss = []
        word_no = -1
        for sent in tqdm(tokens, desc="Combining predicted tags", disable=disable_progress_bar):
            for word in sent:
                word_no += 1
                possible_tags = self.possible_tags
                if self.lexicon is not None and word in self.lexicon:
                    possible_tags = self.lexicon[word]
                if possible_tags is not None:
                    tag_probs = self.calc_tag_probs(possible_tags, preds, word_no)
                    top_prediction = tag_probs[0]
                    best_tags.append(top_prediction)
                    if return_all_probs:
                        all_tags.append(tag_probs)
                    if return_num_poss:
                        num_poss.append(len(possible_tags))
                else:
                    tag = []
                    prob = 1
                    # For now, we don't calculate the probability of all combinations if no list of possible tags is
                    # supplied, meaning all_tags will be empty
                    for feat in preds:
                        poss = sorted(preds[feat][word_no].items(), reverse=True, key=lambda x: x[1])
                        best_poss = poss[0]
                        tag.append((feat, best_poss[0]))
                        prob = prob * best_poss[1]
                    tag = tuple(tag)
                    best_tags.append((tag, prob))
                    num_poss.append('NA')
        if return_all_probs and return_num_poss:
            return best_tags, all_tags, num_poss
        elif return_num_poss:
            return best_tags, num_poss
        elif return_all_probs:
            return best_tags, all_tags
        else:
            return best_tags

    def write_prediction(self, wids, tokens, tokens_norm, best_tags, output_file, output_format='CONLLU', num_poss=None,
                         output_gold=True, output_sentence=True):
        if output_gold:
            tags_gold = dict()
            for feat in self.feature_dict:
                if feat == 'UPOS' or feat == 'XPOS':
                    tags_gold[feat] = self.reader.read_tokens(self.test_data, feat, return_tokens=False,
                                                            return_wids=False)
                else:
                    tags_gold[feat] = self.reader.read_tokens(self.test_data, feat, in_feats=True, return_tokens=False,
                                                            return_wids=False)
        with open(output_file, 'w', encoding='UTF-8') as outfile:
            if output_format == 'tab':
                outfile.write("id\ttoken\tprobability\tpossibilities\tin_lexicon")
                for feat in self.feature_dict:
                    outfile.write(f"\t{feat}")
                if output_gold:
                    for feat in self.feature_dict:
                        outfile.write(f"\tgold_{feat}")
                if output_sentence:
                    outfile.write("\tsentence")
                outfile.write('\n')
            word_no = -1
            for sent_id, sent in enumerate(tokens):
                for word_id, word in enumerate(sent):
                    wid = wids[sent_id][word_id]
                    word_no += 1
                    top_prediction = best_tags[word_no]
                    tag = dict(top_prediction[0])
                    upos = "_"
                    xpos = "_"
                    if 'UPOS' in self.feats:
                        upos = tag["UPOS"]
                    if 'XPOS' in self.feats:
                        xpos = tag["XPOS"]
                    if output_format == 'CONLLU':
                        morph = ""
                        for feat, val in tag.items():
                            if feat != 'UPOS' and feat != 'XPOS' and val != '_':
                                morph += feat + "=" + val + "|"
                        if morph == "":
                            morph = '_'
                        else:
                            morph = morph[:-1]
                        outfile.write(f"{wid}\t{word}\t_\t{upos}\t{xpos}\t{morph}\t_\t_\t_\t_\n")
                    elif output_format == 'tab':
                        if self.lexicon is not None:
                            word_norm = tokens_norm[sent_id][word_id]
                            outfile.write(f"{wid}\t{word}\t{top_prediction[1]:.5f}\t{num_poss[word_no]}\t{word_norm in self.lexicon}")
                        else:
                            outfile.write(f"{wid}\t{word}\t{top_prediction[1]:.5f}\t{num_poss[word_no]}\tNA")
                        for feat in self.feature_dict:
                            val = '_'
                            if feat in tag:
                                val = tag[feat]
                            outfile.write(f"\t{val}")
                        if output_gold:
                            for feat in self.feature_dict:
                                outfile.write(f'\t{tags_gold[feat][sent_id][word_id]}')
                        if output_sentence:
                            sent_str = ''
                            for word_id_2, word_2 in enumerate(sent):
                                if word_id_2 == word_id:
                                    sent_str += '['
                                sent_str += word_2
                                if word_id_2 == word_id:
                                    sent_str += ']'
                                sent_str += ' '
                            outfile.write(f'\t{sent_str.strip()}')
                        outfile.write('\n')
                if output_format == 'CONLLU':
                    outfile.write('\n')

    def prediction_string(self, tokens, wids, all_tags):
        word_no = -1
        output = ""
        for sent_id, sent in enumerate(tokens):
            for word_id, word in enumerate(sent):
                wid = wids[sent_id][word_id]
                word_no += 1
                tag_probs = all_tags[word_no]
                top_prediction = tag_probs[0]
                tag = dict(top_prediction[0])
                second_tag = {}
                if len(tag_probs) > 1:
                    second_prediction = tag_probs[1]
                    second_tag = dict(second_prediction[0])
                output += ('{0:.3f}'.format(
                    top_prediction[1]) + '&nbsp;&nbsp;&nbsp;&nbsp;' + '<b><font style="' + self.color_by_prob(
                    top_prediction[1]) + '">' + word + '</font></b>' + '&nbsp;&nbsp;&nbsp;&nbsp;' + str(
                    word in self.lexicon) + '&nbsp;&nbsp;&nbsp;&nbsp;' + str(tag) + '&nbsp;&nbsp;&nbsp;&nbsp;' + str(
                    second_tag) + '<br>')
        return output
    
    def predictions_table(self,best_tags,tokens):
        table = {}
        flattened_tokens = sum(tokens,[])
        for index, word in enumerate(best_tags):
            if 'form' in table:
                form_vals = table['form']
            else:
                form_vals = []
            form_vals.append(flattened_tokens[index])
            table['form'] = form_vals
            tag = word[0]
            prob = word[1]
            for feat in tag:
                key = feat[0]
                val = feat[1]
                if key in table:
                    feature_vals = table[key]
                else:
                    feature_vals = []
                feature_vals.append(val)
                table[key] = feature_vals
            if 'probability' in table:
                prob_vals = table['probability']
            else:
                prob_vals = []
            prob_vals.append(prob)
            table['probability'] = prob_vals
        return table 

    def read_lexicon(self, file, normalization_rule=None):
        lexicon = {}
        file = open(file, encoding='utf-8')
        raw_text = file.read().strip()
        lines = raw_text.split('\n')
        header = lines.pop(0).split('\t')
        feat_col = {}
        for col, feat in enumerate(header):
            feat_col[feat] = col
        for line in lines:
            entry = line.split('\t')
            form = entry[0]
            if normalization_rule is not None:
                form = Tokenization.normalize_token(form, normalization_rule)
            tag = []
            for feat in self.feature_dict:
                tag.append((feat, entry[feat_col[feat]]))
            tag = tuple(tag)
            if form in lexicon:
                tags = lexicon[form]
                if tag not in tags:
                    tags.append(tag)
            else:
                tags = []
                tags.append(tag)
                lexicon[form] = tags
        return lexicon

    def trim_lexicon(self):
        # Removes unnecessary information and sorts the tag in the order of the feature dict
        lexicon_new = {}
        for form, tags in self.lexicon.items():
            new_tags = []
            for tag in tags:
                new_tag = []
                for feat in self.feature_dict:
                    for tag_feat in tag:
                        if tag_feat[0] == feat:
                            new_tag.append(tag_feat)
                new_tag = tuple(new_tag)
                new_tags.append(new_tag)
            lexicon_new[form] = new_tags
        self.lexicon = lexicon_new

    def read_string(self, string, lang='greek_glaux'):
        if lang == 'greek_glaux':
            tokens_str = Tokenization.greek_glaux_to_tokens(string)
        else:
            tokens_str = string.split(' ')
        tokens = []
        current_sent = []
        new_sent = False
        for token in tokens_str:
            if lang == 'greek_glaux':
                token = ud.normalize("NFKD", token)
                token = Tokenization.normalize_greek_punctuation(token)
            if new_sent:
                tokens.append(current_sent)
                current_sent = []
                new_sent = False
            current_sent.append(token)
            if lang == 'greek_glaux' or lang == 'greek':
                if token == '.' or token == ';' or token == '·':
                    new_sent = True
            else:
                if token == '.' or token == '?' or token == '!':
                    new_sent = True
        tokens.append(current_sent)
        wids = []
        for sent in tokens:
            wids_sent = []
            for id, token in enumerate(sent):
                wids_sent.append(str(id + 1))
            wids.append(wids_sent)
        return wids, tokens

    def color_by_prob(self, prob):
        green = 200 * prob
        red = 200 * (1 - prob)
        return "color: rgb(" + f'{red:.0f}' + "," + f'{green:.0f}' + ",0)"


if __name__ == '__main__':
    arg_parser = ArgumentParser()
    arg_parser.add_argument('mode', help='train/test')
    arg_parser.add_argument('transformer_path', help='path to the transformer model')
    arg_parser.add_argument('model_dir', help='path of tagging models')
    arg_parser.add_argument('--training_data', help='tagger training data')
    arg_parser.add_argument('--test_data', help='tagger test data')
    arg_parser.add_argument('--feats',
                            help='features to be extracted from the CONLL, default UPOS,XPOS,FEATS (any specific '
                                 'values in the FEATS column are also possible)',
                            default='UPOS,XPOS,FEATS')
    arg_parser.add_argument('--tokenizer_path',
                            help='path to the tokenizer (defaults to the path of the transformer model)')
    arg_parser.add_argument('--output_file', help='tagged data')
    arg_parser.add_argument('--output_format',
                            help='format of the output data: CONLLU or tab ('
                                 'tabular format, with tag probability, number possible tags, whether the tag occurs '
                                 'in the lexicon, and without sentence boundaries)',
                            default='CONLLU')
    arg_parser.add_argument('--data_preset', help="format of the input data: default is CONLLU",
                            type=str, default='CONLLU')
    arg_parser.add_argument('--feature_cols',help='define a custom format for the data, e.g. {"ID":0,"FORM":2,"MISC":3}')
    arg_parser.add_argument('--unknown_label',
                            help='tag in the test data for tokens for which we do not know the label beforehand')
    arg_parser.add_argument('--is_multitask', help='use option to signal multitask tagging, '
                                               'otherwise separate models',
                            default=False, action=argparse.BooleanOptionalAction)
    arg_parser.add_argument('--possible_tags_file',
                            help='file containing all possible morphology combinations that are linguistically valid')
    arg_parser.add_argument('--add_td_to_possible_tags', help='constrain the possible tags to the ones occurring in the training data',
                            default=True, action=argparse.BooleanOptionalAction)
    arg_parser.add_argument('--lexicon', help='file containing morphological lexicon')
    arg_parser.add_argument('--normalization_rule',
                            help='normalize tokens during training/testing, normalization rules implemented are '
                                 'greek_glaux and standard NFD/NFKD/NFC/NFKC')
    arg_parser.add_argument('--epochs', help='number of epochs for training, defaults to 3',
                            type=int, default=3)
    arg_parser.add_argument('--freeze_epochs', help='number of frozen epochs for training, defaults to 0',
                            type=int, default=0)
    arg_parser.add_argument('--learning_rate',help='learning rate for training, defaults to 5e-5',type=float,default=5e-5)
    arg_parser.add_argument('--batch_size', help='batch size for training/testing, defaults to 16',
                            type=int, default=16)
    arg_parser.add_argument('--tokenizer_add_prefix_space',
                            help='use option add_prefix_space for tokenizer (necessary for RobertaTokenizerFast)',
                            default=False, action=argparse.BooleanOptionalAction)

    args = arg_parser.parse_args()
    feats = None
    if args.feats is not None:
        feats = args.feats.split(',')
    feature_cols = args.feature_cols
    if feature_cols is not None:
        # Only required for Eclipse
        feature_cols = json.loads(feature_cols.replace('\'','"'))
        # Other
        # feature_cols = json.loads(feature_cols)

    if args.mode == 'train':
        if args.training_data is None:
            print('Training data is missing')
        else:
            tagger = Tagger(training_data=args.training_data, tokenizer_path=args.tokenizer_path,
                            transformer_path=args.transformer_path, feats=feats, model_dir=args.model_dir,
                            data_preset=args.data_preset, is_multitask=args.is_multitask,feature_cols=feature_cols, add_training_data_to_possible_tags=args.add_td_to_possible_tags)
            tokens = tagger.reader.read_tokens(data=tagger.training_data, return_wids=False,
                                             return_tags=False)
            tagger.train_models(tokens, batch_size=args.batch_size, epochs=args.epochs, learning_rate=args.learning_rate, freeze_epochs=args.freeze_epochs,
                                normalization_rule=args.normalization_rule,
                                tokenizer_add_prefix_space=args.tokenizer_add_prefix_space)
    elif args.mode == 'test':
        if args.test_data is None:
            print('Test data is missing')
        else:
            tagger = Tagger(training_data=args.training_data, test_data=args.test_data,
                            tokenizer_path=args.tokenizer_path, transformer_path=args.transformer_path, feats=feats,
                            model_dir=args.model_dir, unknown_label=args.unknown_label, lexicon_file=args.lexicon,
                            possible_tags_file=args.possible_tags_file, data_preset=args.data_preset,
                            is_multitask=args.is_multitask,feature_cols=feature_cols, add_training_data_to_possible_tags=args.add_td_to_possible_tags)
            wids, tokens = tagger.reader.read_tokens(data=tagger.test_data, return_tags=False)
            tokens_norm = tokens

            if args.normalization_rule is not None:
                tokens_norm = Tokenization.normalize_tokens(tokens, args.normalization_rule)
            all_preds = tagger.tag_seperately(tokens_norm, batch_size=args.batch_size, tokenizer_add_prefix_space=args.tokenizer_add_prefix_space)
            best_tags, num_poss = tagger.tag_data(tokens_norm, all_preds, False, True)
            if args.output_file is not None:
                tagger.write_prediction(wids, tokens, tokens_norm, best_tags, args.output_file, args.output_format,
                                        num_poss)
